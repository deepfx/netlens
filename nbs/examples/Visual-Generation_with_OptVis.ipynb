{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import models\n",
    "from netlens.modules import FlatModel\n",
    "from netlens.data import *\n",
    "from netlens.image_proc import *\n",
    "from netlens.visualization import OptVis, ImageParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../../images/examples/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.vgg11(pretrained=True)\n",
    "network.to('cuda' if torch.cuda.is_available() else 'cpu');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlatModel.from_nested_cnn(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  IDX | KEY                       | LAYER\n",
      "--------------------------------------------------------------------------------\n",
      "    0 | features-conv-0           | Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    1 | features-relu-0           | ReLU()\n",
      "    2 | features-pool-0           | MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    3 | features-conv-1           | Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    4 | features-relu-1           | ReLU()\n",
      "    5 | features-pool-1           | MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    6 | features-conv-2           | Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    7 | features-relu-2           | ReLU()\n",
      "    8 | features-conv-3           | Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    9 | features-relu-3           | ReLU()\n",
      "   10 | features-pool-2           | MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "   11 | features-conv-4           | Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "   12 | features-relu-4           | ReLU()\n",
      "   13 | features-conv-5           | Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "   14 | features-relu-5           | ReLU()\n",
      "   15 | features-pool-3           | MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "   16 | features-conv-6           | Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "   17 | features-relu-6           | ReLU()\n",
      "   18 | features-conv-7           | Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "   19 | features-relu-7           | ReLU()\n",
      "   20 | features-pool-4           | MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "   21 | avgpool-0                 | AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "   22 | flatten                   | Lambda()\n",
      "   23 | classifier-linear-0       | Linear(in_features=25088, out_features=4096, bias=True)\n",
      "   24 | classifier-relu-0         | ReLU()\n",
      "   25 | classifier-dropout-0      | Dropout(p=0.5, inplace=False)\n",
      "   26 | classifier-linear-1       | Linear(in_features=4096, out_features=4096, bias=True)\n",
      "   27 | classifier-relu-1         | ReLU()\n",
      "   28 | classifier-dropout-1      | Dropout(p=0.5, inplace=False)\n",
      "   29 | classifier-linear-2       | Linear(in_features=4096, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fASOhUQFiqTm"
   },
   "source": [
    "## Class Visualisations\n",
    "\n",
    "First we generate visualisations of the classes. To change the class selected, simply change the value of `neuron`.\n",
    "\n",
    "We parameterise the input noise in the colour decorrelated, Fourier domain. This helps create better visualisations. For more, see: https://distill.pub/2017/feature-visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94HJ34BQiqTo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [50], loss=0.3482\n",
      "Run [100], loss=-0.1323\n",
      "Run [150], loss=-0.1426\n",
      "Run [200], loss=-0.1421\n",
      "Run [250], loss=-0.1411\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABLUlEQVR4nO3RQREAIAzAMMD4rE9GHjQKetc7MyfO0wG/awDWAKwBWAOwBmANwBqANQBrANYArAFYA7AGYA3AGoA1AGsA1gCsAVgDsAZgDcAagDUAawDWAKwBWAOwBmANwBqANQBrANYArAFYA7AGYA3AGoA1AGsA1gCsAVgDsAZgDcAagDUAawDWAKwBWAOwBmANwBqANQBrANYArAFYA7AGYA3AGoA1AGsA1gCsAVgDsAZgDcAagDUAawDWAKwBWAOwBmANwBqANQBrANYArAFYA7AGYA3AGoA1AGsA1gCsAVgDsAZgDcAagDUAawDWAKwBWAOwBmANwBqANQBrANYArAFYA7AGYA3AGoA1AGsA1gCsAVgDsAZgDcAagDUAawDWAKwBWAOwBmANwBaSdwJ9MSU46QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128 at 0x16D9EEF60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [300], loss=-0.1405\n",
      "Run [350], loss=-0.1395\n",
      "Run [400], loss=-0.1384\n",
      "Run [450], loss=-0.1380\n",
      "Run [500], loss=-0.1371\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABLUlEQVR4nO3RQREAIAzAMMD4rE9GHjQKetc7MyfO0wG/awDWAKwBWAOwBmANwBqANQBrANYArAFYA7AGYA3AGoA1AGsA1gCsAVgDsAZgDcAagDUAawDWAKwBWAOwBmANwBqANQBrANYArAFYA7AGYA3AGoA1AGsA1gCsAVgDsAZgDcAagDUAawDWAKwBWAOwBmANwBqANQBrANYArAFYA7AGYA3AGoA1AGsA1gCsAVgDsAZgDcAagDUAawDWAKwBWAOwBmANwBqANQBrANYArAFYA7AGYA3AGoA1AGsA1gCsAVgDsAZgDcAagDUAawDWAKwBWAOwBmANwBqANQBrANYArAFYA7AGYA3AGoA1AGsA1gCsAVgDsAZgDcAagDUAawDWAKwBWAOwBmANwBaSdwJ9MSU46QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128 at 0x16DAC4390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select a layer from the network\n",
    "layer = 'classifier-linear-2'\n",
    "neuron = 5\n",
    "\n",
    "# Create an OptVis object from a PyTorch model\n",
    "optvis = OptVis.from_activations(model, layer=layer, neuron=neuron, optim_params={'lr': 0.05, 'weight_decay': 5.0})\n",
    "\n",
    "# Parameterise input noise raw\n",
    "img_param = ImageParam(size=128, fft=False, decorrelate=False)\n",
    "\n",
    "# Create visualisation\n",
    "output = optvis.vis(img_param, thresh=(250, 500), transform=False, verbose=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAAydJREFUeJzt2rENAzEMBEG94cLZOd3Ap4aCnQkVXbRgoGd3D9D1uT0AuEsEIE4EIE4EIE4EIE4EIE4EIE4EIO57e8A558yMH0vwZzPzvL27BCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCBOBCDu2d3bG4CLXAIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQJwIQ9wMHugvxHqGlJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterise input noise in colour decorrelated Fourier domain\n",
    "img_param = ImageParam(size=128, fft=True, decorrelate=True)\n",
    "\n",
    "# Create visualisation\n",
    "output = optvis.vis(img_param, thresh=(250, 500), transform=True, verbose=True, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWvYepe2iqTt"
   },
   "source": [
    "## Channel Visualisations\n",
    "\n",
    "Now let's generate some visualisations of the channels of the convolutional layers of the network. We can see the names and number of channels of each layer by using the `get_layer_names()` method. We can then generate channel visualisations in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dn0SRvviqT9"
   },
   "outputs": [],
   "source": [
    "# Select a layer from the network\n",
    "layer = 'features-conv-6'\n",
    "\n",
    "# Choose a channel that is within the size of the layer\n",
    "channel = 32\n",
    "\n",
    "# Create an OptVis object from a PyTorch model\n",
    "optvis = OptVis.from_activations(model, layer=layer, channel=channel, optim_params={'lr': 0.05, 'weight_decay': 0.03})\n",
    "\n",
    "# Parameterise input noise raw\n",
    "img_param = ImageParam(size=224, fft=False, decorrelate=False)\n",
    "\n",
    "# Create visualisation\n",
    "output = optvis.vis(img_param, thresh=(250, 500), transform=False, verbose=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterise input noise in colour decorrelated Fourier domain\n",
    "img_param = ImageParam(size=224, fft=True, decorrelate=True)\n",
    "\n",
    "# Create visualisation\n",
    "output = optvis.vis(img_param, thresh=(250, 500), transform=True, verbose=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
